{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenDiffIt\n",
    "\n",
    "Compares old files with new files. Tracks progress from old data to new data. Checks new documents for accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "Enter location of .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_old_file = \"example/findfiles-result_january.csv\"\n",
    "location_of_new_file = \"example/findfiles-result_february.csv\"\n",
    "location_for_updated_file = \"example/findfiles-result_january-february-diff.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Location of .csv files with extra metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_extra_cols_file = \"example/results_extra.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: \n",
    "Choose ```Kernel > Restart Kernel and Run All Cells...``` from the menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Set max rows to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Import .csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Import old data\n",
    "\n",
    "Note: If you have no old data to compare you still must have a file with column names that inclue 'url' and 'hash'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.read_csv(location_of_old_file, dtype=str)\n",
    "df_old.replace(np.nan, '...', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.drop_duplicates(subset=['url'], inplace=True)\n",
    "df_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(location_of_new_file, dtype=str)\n",
    "df_new.replace(np.nan, '...', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.drop_duplicates(subset=['url'], inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Import new current data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Create and Carry over and to new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - OpenDiffIt: Create unique 'hash' for each file in .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "def get_remote_sha_sum(url):\n",
    "    url = url.split('?')[0]\n",
    "    print('OpenDiffIt: Hashing file ' + url.rsplit('/', 1)[-1].split('?')[0] + ' ...')\n",
    "    \"\"\" Put remote file in memory and create hash \"\"\"\n",
    "    MAXSIZE = 26214400 # 25MB\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if len(response.content) < MAXSIZE:\n",
    "                sha1 = hashlib.sha1()\n",
    "                response = response.content\n",
    "                sha1.update(response)\n",
    "                return sha1.hexdigest()\n",
    "            else:\n",
    "                logging.info('Skipping %s because  %s MB is really big.' % (url, str(MAXSIZE/819200)))\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print('1')\n",
    "            print(\"%(error)s:\" % dict(error=e))\n",
    "            return e\n",
    "    \n",
    "    else:\n",
    "        return \"Status \" + str(response.status_code)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hash for each file\n",
    "    \n",
    "df_new['hash'] = df_new['url'].apply(get_remote_sha_sum)\n",
    "# Discard rows with bad urls\n",
    "df_new = df_new[~df_new['hash'].str.contains(\"Error\", na=True)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Carry over all column headers from the old spreadsheet to the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQ_COLS = ['diff','comply','notes'] # Required cols\n",
    "EXPIRED_COLS = ['diff', 'count', 'hash'] # Cols with expired data to exclude\n",
    "existing_cols = list(set().union(df_old.columns, df_new.columns)) # All cols in both spreads\n",
    "custom_cols = [col for col in existing_cols if col not in EXPIRED_COLS] # Cols that do not exist\n",
    "all_cols = list(set().union(existing_cols, REQ_COLS))\n",
    "\n",
    "for col in all_cols:\n",
    "    if col not in df_new.columns:\n",
    "        df_new[col] = \"...\"\n",
    "                        \n",
    "df_new.replace(np.nan, '...', inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Compare old files to new files using 'hash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_old(row):\n",
    "    ''' Compare old hash to new hash and migrate relevent data '''    \n",
    "    is_old = df_old[ df_old['url'] == row['url'] ]\n",
    "        \n",
    "    if is_old.empty: \n",
    "        row['diff'] = 'NEW'\n",
    "        row['comply'] = 'UNKNOWN'\n",
    "        row['notes'] = '...'\n",
    "    elif is_old.iloc[0]['comply'] == 'SKIP':\n",
    "        row['diff'] = 'SKIP'\n",
    "        row['comply'] = 'SKIP'\n",
    "        row['notes'] = 'Not relevent.'\n",
    "    else:\n",
    "            \n",
    "        if (is_old.iloc[0]['hash'] == row['hash']):\n",
    "            for col in custom_cols:\n",
    "                row[col] = is_old.iloc[0][col] or '...'\n",
    "            row['diff'] = 'SAME'\n",
    "        elif is_old.iloc[0]['hash'] != row['hash']:\n",
    "            for col in custom_cols:\n",
    "                row[col] = is_old.iloc[0][col] or '...'\n",
    "            row['diff'] = 'UPDATED'\n",
    "            row['comply'] = 'UNKNOWN'\n",
    "            row['notes'] = '...'\n",
    "        else:\n",
    "            row['diff'] = 'IDK'\n",
    "            row['comply'] = 'IDK'\n",
    "            row['notes'] = '...'\n",
    "    return row\n",
    "\n",
    "df_new.apply(compare_to_old, axis=1)\n",
    "df_new.replace(np.nan, '...', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move specific required columns to the end togther\n",
    "cols_at_end = REQ_COLS\n",
    "df_new = df_new[[c for c in df_new if c not in cols_at_end]  + [c for c in cols_at_end if c in df_new]]\n",
    "df_new.replace(np.nan, '...', inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Check 'NEW' and 'UPDATED' files for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import wget\n",
    "import logging\n",
    "from pdfminer3.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer3.pdfdevice import TagExtractor\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "from urllib.parse import unquote\n",
    "    \n",
    "    \n",
    "def detect_tags(url):\n",
    "    \"\"\" Detect if PDF has proper tags \"\"\"\n",
    "    \n",
    "    tags = [\"<b\\'Part\", \"<b\\'Sect\", \"<b\\'Art\", \"<b\\'Content\", \"<b\\'Index\", \"<b\\'BibEntry\", \"<b\\'Lbl\", \"<b\\'Index\", \"<b\\'Note\", \"<b\\'Reference\", \"<b\\'Figure\", \"<b\\'Artifact\", \"<b\\'ArtifactSpan\", \"<b\\'LBody\", \"<b\\'Normal\", \"<b\\'Heading 1\", \"<b\\'Heading 2\", \"<b\\'H1\", \"<b\\'H2\", \"<b\\'Table\", \"<b\\'P\", \"\\'Annots\"]\n",
    "\n",
    "    clean_url = unquote(url)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    \n",
    "    if clean_url.endswith('.pdf'):\n",
    "    \n",
    "\n",
    "        try:\n",
    "            device = TagExtractor(rsrcmgr, retstr, codec='utf-8')\n",
    "        except UnicodeError as ex:\n",
    "            device = TagExtractor(rsrcmgr, retstr, codec='ascii')\n",
    "\n",
    "        file_name = clean_url.rsplit('/', 1)[-1].split('?')[0]\n",
    "\n",
    "        temp_download_file_location = os.path.join('tmp', file_name)\n",
    "\n",
    "        if os.path.exists(temp_download_file_location):\n",
    "            print(\"OpenDiffIt: \" + file_name + \" Using local cached file.\")\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                temp_download_file_location = wget.download(clean_url, temp_download_file_location)\n",
    "                print(\"OpenDiffIt: \" + file_name + \" Streaming new file from server...\")\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "        try:\n",
    "            # Open the file\n",
    "            with open(temp_download_file_location, 'rb') as fp:\n",
    "                print('OpenDiffIt: Checking ' + file_name + ' for tags...')\n",
    "\n",
    "                fp_size = os.path.getsize(temp_download_file_location)\n",
    "\n",
    "                MAXSIZE = 2306866 # 22MB\n",
    "\n",
    "                if fp_size < MAXSIZE:\n",
    "                    logging.info('File is less than 22 MB. Try to detect.')\n",
    "\n",
    "                    if (fp_size < MAXSIZE / 2):\n",
    "                        maxpages = 2\n",
    "                    elif (fp_size < MAXSIZE / 4):\n",
    "                        maxpages = 4\n",
    "                    elif (fp_size < MAXSIZE / 8):\n",
    "                        maxpages = 8\n",
    "                    else:\n",
    "                        maxpages = 1\n",
    "\n",
    "                    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "                    password = ''\n",
    "                    caching = True\n",
    "                    pagenos=set()\n",
    "                    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "                        interpreter.process_page(page)\n",
    "\n",
    "                    contents = retstr.getvalue().decode()\n",
    "                    device.close() # check if these need to be here still context manager stuff\n",
    "                    retstr.close() # check if these need to be here still\n",
    "\n",
    "                    try:\n",
    "                        if any(item in contents for item in tags):\n",
    "                            return label_comply(contents)\n",
    "                        else:\n",
    "                            return 'NO', 'Needs Tagged.'\n",
    "                        \n",
    "                    except Exception as ex:\n",
    "                        print(ex)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        print('OpenDiffIt: Too big to scan.')\n",
    "                        return 'UNKNOWN', 'Too big to scan.'\n",
    "                    except Exception as ex:\n",
    "                        print(ex)\n",
    "                \n",
    "            try:\n",
    "                os.remove(temp_download_file_location)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "    else:\n",
    "        return 'Not PDF', 'Manually check.'\n",
    "\n",
    "def label_comply(contents):\n",
    "    \"\"\"examine the contents of the file\"\"\"\n",
    "    \n",
    "    try:\n",
    "        msg = \"Is Tagged. \"\n",
    "\n",
    "        if (\"<b'H\" in contents):\n",
    "            msg = msg + \" And has a Heading Tag.\"\n",
    "            logging.info(msg)                    \n",
    "            status = 'MAYBE'\n",
    "            notes = msg\n",
    "\n",
    "        else:\n",
    "            msg = msg + \" But needs a Heading Tag. Other issues possible.\"\n",
    "            logging.info(msg)\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        if (\"<b'Table\" in contents) and (\"<b'TH\" not in contents):\n",
    "            msg = msg + \" At least one Table is missing TH.\"\n",
    "            logging.info(msg)\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        if \"_____\" in contents:\n",
    "            msg = msg + \" Probably a Form with issues.\"\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        if \".....\" in contents:\n",
    "            msg = msg + \" Probably has a table of contents with issues.\"\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        return status, msg\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_new\n",
    "same = ['SAME', 'SKIP']\n",
    "other = ['UPDATED', 'NEW', 'IDK']\n",
    "df_merged_same = df_merged[df_merged.loc[:,'diff'].isin(same)]\n",
    "df_merged_other = df_merged[df_merged.loc[:,'diff'].isin(other)]\n",
    "\n",
    "\n",
    "df_merged_other.loc[:,'comply'],df_merged_other.loc[:,'notes'] = zip(*df_merged_other.loc[:,\"url\"].map(detect_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_merged_same, df_merged_other]).sort_index(ascending=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - _OPTIONAL: Add link and Scrapy location metadata_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra = pd.read_csv(location_of_extra_cols_file, dtype=str)\n",
    "df_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.merge(df_extra, how=\"left\", on=\"url\")\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Replace empty cells with elipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.replace(np.nan, '...', inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Export dataframe as new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[['count','url','hash','link_text','from_page_url','diff','comply','notes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(location_for_updated_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - OpenDiffIt: Export XLSX file with color coded conditional formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xlsxwriter\n",
    "\n",
    "def csv_to_xlsx(csv_file):\n",
    "    \"\"\" Convert csv to xlsx with formating \"\"\"\n",
    "    \n",
    "    # Calulate column letter for 'comply' column\n",
    "    num_of_cols = len(df_final.columns)\n",
    "    comply_col_index = num_of_cols - 1\n",
    "    comply_col_letter = chr(ord('@')+comply_col_index)\n",
    "        \n",
    "    wb = xlsxwriter.Workbook(csv_file[:-4] + '.xlsx')\n",
    "    ws = wb.add_worksheet(\"WS1\")    # your worksheet title here\n",
    "    # ws.insert_textbox('B2', 'Edit using Online Excel in Box!', {'width': 256, 'height': 100})\n",
    "    ws.insert_textbox('G1', 'Only edit using Online Excel in Box!',\n",
    "                         {'width': 250,\n",
    "                          'height': 30,\n",
    "                          'y_offset': 25,\n",
    "                          'x_offset': 25,\n",
    "                          'font': {'bold': True,'color': 'red'},\n",
    "                          'align': {'vertical': 'middle','horizontal': 'center'},\n",
    "                          'line': {'color': 'red','width': 1.25,'dash_type': 'square_dot'}})\n",
    "\n",
    "    formatyellow = wb.add_format({'bg_color':'#FFD960'})\n",
    "    formatpink = wb.add_format({'bg_color':'#ffc0cb'})\n",
    "    formatgreen = wb.add_format({'bg_color':'#ccff80'})\n",
    "    formatgrey = wb.add_format({'bg_color':'#676767'})\n",
    "\n",
    "    # TODO: Do something with goofy character issues other than ignore errors\n",
    "    with open(csv_file,'r', encoding='utf-8', errors='ignore') as csvfile:\n",
    "        \"\"\" Convert csv to xlsx with formating \"\"\"\n",
    "        table = csv.reader(csvfile)\n",
    "        i = 0\n",
    "        # write each row from the csv file as text into the excel file\n",
    "        # this may be adjusted to use 'excel types' explicitly (see xlsxwriter doc)\n",
    "        for row in table:\n",
    "            ws.write_row(i, 0, row)\n",
    "            i += 1\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"UNKNOWN\"',\n",
    "                      'format':formatyellow})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"MAYBE\"',\n",
    "                      'format':formatyellow})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"PENDING\"',\n",
    "                      'format':formatyellow})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"NO\"',\n",
    "                      'format':formatpink})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"YES\"',\n",
    "                      'format':formatgreen})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"SKIP\"',\n",
    "                      'format':formatgreen})\n",
    "\n",
    "        ws.set_column(0, 0, 50) # url\n",
    "        ws.set_column(1, 1, 50) # link text\n",
    "        ws.freeze_panes(1, 0)\n",
    "    logging.info('Converted csv to pretty xlsx')\n",
    "    wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_xlsx(location_for_updated_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User step 3: Get your resulting CSV and XLSX file\n",
    "\n",
    "Result file is in the following relative location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_for_updated_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
