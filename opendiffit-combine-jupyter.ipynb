{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenDiffIt\n",
    "\n",
    "Compares old files with new files. Tracks progress from old data to new data. Checks new documents for accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User step 1: \n",
    "Enter location of .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_old_file = \"combine/combine-extracolumns.csv\"\n",
    "location_of_new_data_file = \"combine/normal-file.csv\"\n",
    "location_for_updated_file = \"example/combine-with-new.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User step 2: \n",
    "Choose ```Kernel > Restart Kernel and Run All Cells...``` from the menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenDiffIt automation:\n",
    "Sit back a while and let OpenDiffIt do the magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from -r ./requirements.txt (line 1)) (2.20.0)\n",
      "Requirement already satisfied: wget==3.2 in /opt/anaconda3/lib/python3.8/site-packages (from -r ./requirements.txt (line 2)) (3.2)\n",
      "Requirement already satisfied: pdfminer3==2018.12.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from -r ./requirements.txt (line 3)) (2018.12.3.0)\n",
      "Requirement already satisfied: xlrd==1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from -r ./requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: xlsxwriter==1.1.8 in /opt/anaconda3/lib/python3.8/site-packages (from -r ./requirements.txt (line 5)) (1.1.8)\n",
      "Requirement already satisfied: configargparse in /opt/anaconda3/lib/python3.8/site-packages (from -r ./requirements.txt (line 6)) (1.2.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests==2.20.0->-r ./requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests==2.20.0->-r ./requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests==2.20.0->-r ./requirements.txt (line 1)) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests==2.20.0->-r ./requirements.txt (line 1)) (2020.6.20)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer3==2018.12.3.0->-r ./requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: pycryptodome in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer3==2018.12.3.0->-r ./requirements.txt (line 3)) (3.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./requirements.txt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Import old data\n",
    "\n",
    "If you have no old data to compare you still must have a file with column names that inclue 'url' and 'hash'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'combine/combine-extracolumns.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5c8df7bfc266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_of_old_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'combine/combine-extracolumns.csv'"
     ]
    }
   ],
   "source": [
    "df_old = pd.read_csv(location_of_old_file, dtype=str)\n",
    "\n",
    "df_old.replace(np.nan, '...', inplace=True)\n",
    "df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Import new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'combine/normal-file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-472c398cc7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_of_new_data_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'combine/normal-file.csv'"
     ]
    }
   ],
   "source": [
    "df_new = pd.read_csv(location_of_new_data_file, dtype=str)\n",
    "\n",
    "df_new.replace(np.nan, '...', inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate CSVs for required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'url' not in df_new.columns:\n",
    "    print(\"STOP: \" + location_of_new_data_file + \" missing required 'url' column OR missing a column header OR special chars.\")\n",
    "else:\n",
    "    print(\"OK: \" + location_of_new_data_file + \" seems legit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"OK: \" + location_of_old_file + \" seems legit.\"\n",
    "\n",
    "if 'url' not in df_old.columns:\n",
    "    status = \"STOP: \" + location_of_old_file + \" missing required 'url' column OR missing a column header OR has special chars. \"\n",
    "    \n",
    "if 'hash' not in df_old.columns:\n",
    "    status = status + \"WARNING: \" + location_of_old_file + \" missing required 'hash' column to compare against.\"\n",
    "\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Create unique 'hash' for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "def get_remote_sha_sum(url):\n",
    "    url = url.split('?')[0]\n",
    "    print('OpenDiffIt: Hashing file ' + url.rsplit('/', 1)[-1].split('?')[0] + ' ...')\n",
    "    \"\"\" Put remote file in memory and create hash \"\"\"\n",
    "    MAXSIZE = 26214400 # 25MB\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if len(response.content) < MAXSIZE:\n",
    "                sha1 = hashlib.sha1()\n",
    "                response = response.content\n",
    "                sha1.update(response)\n",
    "                return sha1.hexdigest()\n",
    "            else:\n",
    "                logging.info('Skipping %s because  %s MB is really big.' % (url, str(MAXSIZE/819200)))\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print('1')\n",
    "            print(\"%(error)s:\" % dict(error=e))\n",
    "            return e\n",
    "        #     except requests.exceptions.HTTPError as e:\n",
    "        #         print('1')\n",
    "        #         print(\"%(error)s:\" % dict(error=e))\n",
    "        #         return e\n",
    "        #     except requests.exceptions.ConnectionError as e2:\n",
    "        #         print('2')\n",
    "        #         print(\"%(error)s:\" % dict(error=e2))\n",
    "        #         return e\n",
    "        #     except requests.exceptions.SSLError as e3:\n",
    "        #         print('3')\n",
    "        #         print(\"%(error)s:\" % dict(error=e3))\n",
    "        #         return e\n",
    "        #     except requests.exceptions.NewConnectionError as e4:\n",
    "        #         print('4')\n",
    "        #         print(\"%(error)s:\" % dict(error=e4))\n",
    "        #         return e\n",
    "        #     except requests.exceptions.SocketError as e5:\n",
    "        #         print('5')\n",
    "        #         print(\"%(error)s:\" % dict(error=e5))\n",
    "        #         return e\n",
    "        #     except requests.exceptions.MaxRetryError as e6:\n",
    "        #         print('6')\n",
    "        #         print(\"%(error)s:\" % dict(error=e6))\n",
    "        #         return e\n",
    "    \n",
    "    else:\n",
    "        return \"Status \" + str(response.status_code)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hash for each file\n",
    "    \n",
    "df_new['hash'] = df_new['url'].apply(get_remote_sha_sum)\n",
    "# Discard rows with bad urls\n",
    "df_new = df_new[~df_new['hash'].str.contains(\"Error\", na=True)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Carry over all columns from the old spreadsheet to the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQ_COLS = ['diff','comply','notes'] # Required cols\n",
    "EXPIRED_COLS = ['diff', 'count', 'hash'] # Cols with expired data to exclude\n",
    "existing_cols = list(set().union(df_old.columns, df_new.columns)) # All cols in both spreads\n",
    "custom_cols = [col for col in existing_cols if col not in EXPIRED_COLS] # Cols that do not exist\n",
    "all_cols = list(set().union(existing_cols, REQ_COLS))\n",
    "\n",
    "for col in all_cols:\n",
    "    if col not in df_new.columns:\n",
    "        df_new[col] = \"...\"\n",
    "                        \n",
    "df_new.replace(np.nan, '...', inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Compare old files to new files using 'hash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_old(row):\n",
    "    ''' Compare old hash to new hash and migrate relevent data '''    \n",
    "    is_old = df_old[ df_old['url'] == row['url'] ]\n",
    "        \n",
    "    if is_old.empty: \n",
    "        row['diff'] = 'NEW'\n",
    "        row['comply'] = 'UNKNOWN'\n",
    "        row['notes'] = 'x'\n",
    "    elif is_old.iloc[0]['comply'] == 'SKIP':\n",
    "        row['diff'] = 'SKIP'\n",
    "        row['comply'] = 'SKIP'\n",
    "        row['notes'] = 'Not relevent.'\n",
    "    else:\n",
    "            \n",
    "        if (is_old.iloc[0]['hash'] == row['hash']):\n",
    "            for col in custom_cols:\n",
    "                row[col] = is_old.iloc[0][col] or 'x'\n",
    "            row['diff'] = 'SAME'\n",
    "        elif is_old.iloc[0]['hash'] != row['hash']:\n",
    "            for col in custom_cols:\n",
    "                row[col] = is_old.iloc[0][col] or 'x'\n",
    "            row['diff'] = 'UPDATED'\n",
    "            row['comply'] = 'UNKNOWN'\n",
    "            row['notes'] = '...'\n",
    "        else:\n",
    "            row['diff'] = 'IDK'\n",
    "            row['comply'] = 'IDK'\n",
    "            row['notes'] = '...'\n",
    "    return row\n",
    "\n",
    "df_new.apply(compare_to_old, axis=1)\n",
    "df_new.replace(np.nan, '...', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move specific required columns to the end togther\n",
    "cols_at_end = REQ_COLS\n",
    "df_new = df_new[[c for c in df_new if c not in cols_at_end]  + [c for c in cols_at_end if c in df_new]]\n",
    "df_new.replace(np.nan, '...', inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Check 'NEW' and 'UPDATED' files for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import wget\n",
    "import logging\n",
    "from pdfminer3.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer3.pdfdevice import TagExtractor\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "from urllib.parse import unquote\n",
    "    \n",
    "    \n",
    "def detect_tags(url):\n",
    "    \"\"\" Detect if PDF has proper tags \"\"\"\n",
    "    \n",
    "    tags = [\"<b\\'Part\", \"<b\\'Sect\", \"<b\\'Art\", \"<b\\'Content\", \"<b\\'Index\", \"<b\\'BibEntry\", \"<b\\'Lbl\", \"<b\\'Index\", \"<b\\'Note\", \"<b\\'Reference\", \"<b\\'Figure\", \"<b\\'Artifact\", \"<b\\'ArtifactSpan\", \"<b\\'LBody\", \"<b\\'Normal\", \"<b\\'Heading 1\", \"<b\\'Heading 2\", \"<b\\'H1\", \"<b\\'H2\", \"<b\\'Table\", \"<b\\'P\", \"\\'Annots\"]\n",
    "\n",
    "    clean_url = unquote(url)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    \n",
    "    if clean_url.endswith('.pdf'):\n",
    "    \n",
    "\n",
    "        try:\n",
    "            device = TagExtractor(rsrcmgr, retstr, codec='utf-8')\n",
    "        except UnicodeError as ex:\n",
    "            device = TagExtractor(rsrcmgr, retstr, codec='ascii')\n",
    "\n",
    "        file_name = clean_url.rsplit('/', 1)[-1].split('?')[0]\n",
    "\n",
    "        temp_download_file_location = os.path.join('tmp', file_name)\n",
    "\n",
    "        if os.path.exists(temp_download_file_location):\n",
    "            print(\"OpenDiffIt: \" + file_name + \" Using local cached file.\")\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                temp_download_file_location = wget.download(clean_url, temp_download_file_location)\n",
    "                print(\"OpenDiffIt: \" + file_name + \" Streaming new file from server...\")\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "        try:\n",
    "            # Open the file\n",
    "            with open(temp_download_file_location, 'rb') as fp:\n",
    "                print('OpenDiffIt: Checking ' + file_name + ' for tags...')\n",
    "\n",
    "                fp_size = os.path.getsize(temp_download_file_location)\n",
    "\n",
    "                MAXSIZE = 2306866 # 22MB\n",
    "\n",
    "                if fp_size < MAXSIZE:\n",
    "                    logging.info('File is less than 22 MB. Try to detect.')\n",
    "\n",
    "                    if (fp_size < MAXSIZE / 2):\n",
    "                        maxpages = 2\n",
    "                    elif (fp_size < MAXSIZE / 4):\n",
    "                        maxpages = 4\n",
    "                    elif (fp_size < MAXSIZE / 8):\n",
    "                        maxpages = 8\n",
    "                    else:\n",
    "                        maxpages = 1\n",
    "\n",
    "                    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "                    password = ''\n",
    "                    caching = True\n",
    "                    pagenos=set()\n",
    "                    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "                        interpreter.process_page(page)\n",
    "\n",
    "                    contents = retstr.getvalue().decode()\n",
    "                    device.close() # check if these need to be here still context manager stuff\n",
    "                    retstr.close() # check if these need to be here still\n",
    "\n",
    "                    try:\n",
    "                        if any(item in contents for item in tags):\n",
    "                            return label_comply(contents)\n",
    "                        else:\n",
    "                            return 'NO', 'Needs Tagged.'\n",
    "                    except Exception as ex:\n",
    "                        print(ex)\n",
    "\n",
    "                else:\n",
    "                    print('OpenDiffIt: Too big to scan.')\n",
    "                    return 'UNKNOWN', 'Too big to scan.'\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            \n",
    "    else:\n",
    "        return 'Not PDF', 'Manually check.'\n",
    "\n",
    "def label_comply(contents):\n",
    "    \"\"\"examine the contents of the file\"\"\"\n",
    "    \n",
    "    try:\n",
    "        msg = \"Is Tagged. \"\n",
    "\n",
    "        if (\"<b'H\" in contents):\n",
    "            msg = msg + \" And has a Heading Tag.\"\n",
    "            logging.info(msg)                    \n",
    "            status = 'MAYBE'\n",
    "            notes = msg\n",
    "\n",
    "        else:\n",
    "            msg = msg + \" But needs a Heading Tag. Other issues possible.\"\n",
    "            logging.info(msg)\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        if (\"<b'Table\" in contents) and (\"<b'TH\" not in contents):\n",
    "            msg = msg + \" At least one Table is missing TH.\"\n",
    "            logging.info(msg)\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        if \"_____\" in contents:\n",
    "            msg = msg + \" Probably a Form with issues.\"\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        if \".....\" in contents:\n",
    "            msg = msg + \" Probably has a table of contents with issues.\"\n",
    "            status = 'NO'\n",
    "            notes = msg\n",
    "\n",
    "        return status, msg\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_merged = df_new\n",
    "same = ['SAME', 'SKIP']\n",
    "other = ['UPDATED', 'NEW', 'IDK']\n",
    "df_merged_same = df_merged[df_merged['diff'].isin(same)]\n",
    "df_merged_other = df_merged[df_merged['diff'].isin(other)]\n",
    "\n",
    "df_merged_other['comply']['notes'] = zip(*df_merged_other['url'].map(detect_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_new\n",
    "same = ['SAME', 'SKIP']\n",
    "other = ['UPDATED', 'NEW', 'IDK']\n",
    "df_merged_same = df_merged[df_merged.loc[:,'diff'].isin(same)]\n",
    "df_merged_other = df_merged[df_merged.loc[:,'diff'].isin(other)]\n",
    "\n",
    "df_merged_other.loc[:,'comply'],df_merged_other.loc[:,'notes'] = zip(*df_merged_other.loc[:,\"url\"].map(detect_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_merged_same, df_merged_other]).sort_index(ascending=True)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Export dataframe as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(location_for_updated_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenDiffIt: Export XLSX file with color coded conditional formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xlsxwriter\n",
    "\n",
    "def csv_to_xlsx(csv_file):\n",
    "    \"\"\" Convert csv to xlsx with formating \"\"\"\n",
    "    \n",
    "    # Calulate column letter for 'comply' column\n",
    "    num_of_cols = len(df_final.columns)\n",
    "    comply_col_index = num_of_cols - 1\n",
    "    comply_col_letter = chr(ord('@')+comply_col_index)\n",
    "        \n",
    "    wb = xlsxwriter.Workbook(csv_file[:-4] + '.xlsx')\n",
    "    ws = wb.add_worksheet(\"WS1\")    # your worksheet title here\n",
    "    # ws.insert_textbox('B2', 'Edit using Online Excel in Box!', {'width': 256, 'height': 100})\n",
    "    ws.insert_textbox('G1', 'Only edit using Online Excel in Box!',\n",
    "                         {'width': 250,\n",
    "                          'height': 30,\n",
    "                          'y_offset': 25,\n",
    "                          'x_offset': 25,\n",
    "                          'font': {'bold': True,'color': 'red'},\n",
    "                          'align': {'vertical': 'middle','horizontal': 'center'},\n",
    "                          'line': {'color': 'red','width': 1.25,'dash_type': 'square_dot'}})\n",
    "\n",
    "    formatyellow = wb.add_format({'bg_color':'#FFD960'})\n",
    "    formatpink = wb.add_format({'bg_color':'#ffc0cb'})\n",
    "    formatgreen = wb.add_format({'bg_color':'#ccff80'})\n",
    "\n",
    "    # TODO: Do something with goofy character issues other than ignore errors\n",
    "    with open(csv_file,'r', encoding='utf-8', errors='ignore') as csvfile:\n",
    "        \"\"\" Convert csv to xlsx with formating \"\"\"\n",
    "        table = csv.reader(csvfile)\n",
    "        i = 0\n",
    "        # write each row from the csv file as text into the excel file\n",
    "        # this may be adjusted to use 'excel types' explicitly (see xlsxwriter doc)\n",
    "        for row in table:\n",
    "            ws.write_row(i, 0, row)\n",
    "            i += 1\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"UNKNOWN\"',\n",
    "                      'format':formatyellow})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"MAYBE\"',\n",
    "                      'format':formatyellow})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"ALMOST\"',\n",
    "                      'format':formatyellow})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"NO\"',\n",
    "                      'format':formatpink})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"YES\"',\n",
    "                      'format':formatgreen})\n",
    "        ws.conditional_format('A1:XFD1048576', {'type':'formula',\n",
    "                      'criteria':'=INDIRECT(\"' + comply_col_letter + '\"&ROW())=\"SKIP\"',\n",
    "                      'format':formatgreen})\n",
    "\n",
    "        ws.set_column(0, 0, 75) # url\n",
    "        ws.set_column(1, 1, 25) # link text\n",
    "        ws.freeze_panes(1, 0)\n",
    "    logging.info('Converted csv to pretty xlsx')\n",
    "    wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_xlsx(location_for_updated_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User step 3: Get your resulting CSV and XLSX file\n",
    "\n",
    "Result file is in the following relative location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_for_updated_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
